---
title: "CS 362: Research in AI Alignment"
summary: "Course Organizer"  # Add a page description.
date: "2023-03-01T00:00:00Z"  # Add today's date.
type: "teaching"  # Page type is a Widget Page
profile: false
featured: true
weight: 1
showthedate: false
image:
  caption: ""
  focal_point: "TopRight"
  preview_only: false

---

<div style="display: flex; flex-direction: row;">
  <div style="flex: 1;">

## Course Information
* *Instructor Name(s)*: Scott Viteri
* *Teaching Assistants*: Victor Lecomte, Gabe Mukobi, Peter Chatain
* *Course Faculty Sponsor*: Clark Barrett
* *Lecture*: Monday 3:00-4:30pm, B067 Mitchell Earth Science (In-person only)
* *Optional Office Hours*: Large Language Model productivity meetings at 1-2PM in Gates 200
* Graduate-level course or advanced undergraduates (contact course instructor)
* 3 Units, Spring 2023, [ExploreCourses](https://explorecourses.stanford.edu/search?q=CS+362)

  </div>
  <div style="flex: 0 0 30%; margin-left: 20px;">

  <img src="/img/cs362.png" alt="AI Alignment" style="width: 200px;">

  </div>
</div>

## Course Description
In this course we will explore the current state of research in the field of AI alignment, which seeks to bring increasingly intelligent AI systems in line with human values and interests. The purpose of this course is to encourage the development of new ideas in this field, where a dominant paradigm has not yet been established. The format will be weekly lectures in which speakers present their current research approaches.

The assignment structure will be slightly unusual: each week students will have a choice between a problem set and a short research assignment based on the weekly guest speaker’s research area. For the research assignment, students will start with the abstract of a relevant AI alignment paper or blog post and create a blog post or Github repository describing how they would continue the paper. The final weekly assignment will be an extension of one of the previous weeks’ work. Therefore this course requires research experience, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus), and is a graduate level course, open to advanced undergraduates.

## Prerequisites:
Any one of the following: CS223a, CS224n, CS224w, CS224u, CS227b, CS228, CS229, CS229t, CS229m, CS231a, CS231n, CS234, CS236, CS237a, CS281

In addition to the above, strong ability to do independent research will be necessary, preferably using mathematical and programming tools (e.g. Python, PyTorch, calculus).

## Syllabus
| Date | Week | Name | Topic | Suggested Assignment Prompt |
| --- | --- | --- | --- | --- |
| April 3 (Mon) | 1 | Scott Viteri | Overview of Course and AI Safety | [Bowman 2022](https://wp.nyu.edu/arg/why-ai-safety/), [Steinhardt 2022](https://bounded-regret.ghost.io/more-is-different-for-ai/), [Carlsmith 2022](https://www.youtube.com/watch?v=UbruBnv3pZU), [Gates 2022](https://hai.stanford.edu/events/hai-weekly-seminar-vael-gates) |
| April 10 | 4 | Adam Gleave (UC Berkeley) | Inverse Reinforcement Learning | [Gleave, Toyer 2022](https://arxiv.org/pdf/2203.11409.pdf), [Gleave 2022](https://arxiv.org/pdf/2203.07472.pdf) |
| April 17 | 3 | Andrew Critch (UC Berkeley) | Multiagent problems | [Critch 2019](https://www.cambridge.org/core/journals/journal-of-symbolic-logic/article/abs/parametric-resourcebounded-generalization-of-lobs-theorem-and-a-robust-cooperation-criterion-for-opensource-game-theory/16063EA7BFFEE89438631B141E556E79), [Fickinger, Zhuang, Critch et al 2020](https://arxiv.org/pdf/2012.14536.pdf), [Garrabrandt, Critch et al 2016](https://arxiv.org/pdf/1609.03543.pdf) |
| April 24 | 2 | Andy Jones (Anthropic) | Empirical alignment - interpretability | [Askell 2021](https://arxiv.org/abs/2112.00861), [Elhage, Nanda 2021](https://transformer-circuits.pub/2021/framework/index.html) |
| May 1 | 5 | Dan Hendrycks (Center for AI Safety) | Robustness and Generalization in AI Systems | [Hendrycks 2022](https://arxiv.org/abs/2109.13916), [Hendrycks 2021a](https://arxiv.org/abs/2009.03300), [Hendrycks 2021b](https://arxiv.org/abs/2006.16241) |
| May 8 | 6 | Alex Turner (UC Berkeley) | Shard theory | [Turner 2022](https://www.alignmentforum.org/posts/pdaGN6pQyQarFHXF4/reward-is-not-the-optimization-target), [Pope, Turner 2022](https://www.alignmentforum.org/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values) |
| May 15 | 7 | Laria Reynolds (Conjecture) | Empirical alignment research with LLM | [Reynolds, McDonell 2021](https://arxiv.org/pdf/2102.07350.pdf) |
| May 22 | 8 | John Wentworth (independent researcher) | Agent Foundations and Abstractions | [Wentworth 2022a](https://www.lesswrong.com/posts/BzYmJYECAc3xyCTt6/the-plan-2022-update), [Wentworth 2022b](https://www.lesswrong.com/s/TLSzP4xP42PPBctgw) |
| May 29  | 9 | Memorial Day — no class |  |  |
| Jun 5 | 10 | Evan Hubinger (Anthropic) | Mesa-Optimization and Inner Alignment | [Hubinger, Mikulik et al 2019](https://arxiv.org/abs/1906.01820), [Hubinger 2021](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai) |
